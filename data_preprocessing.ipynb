{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a873b1-e1bf-4fb5-a6a9-dd05e4e60bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cdflib\n",
      "  Downloading cdflib-1.3.4-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\z5512000\\appdata\\local\\anaconda3\\envs\\poseestimation\\lib\\site-packages (from cdflib) (1.26.3)\n",
      "Downloading cdflib-1.3.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: cdflib\n",
      "Successfully installed cdflib-1.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install cdflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510209e9-9aac-4ff7-8e70-7f9dd9eeba1a",
   "metadata": {},
   "source": [
    "# To learn:\n",
    "\n",
    "Looking into https://github.com/MLSLab/human36m/blob/main/00_h36m_data_exploration.ipynb,\n",
    "They used 2 preprocessed numpy files, which are,\n",
    "\n",
    "data_3d_h36m.npz: contains 3D poses for all training subjects.\n",
    "\n",
    "data_2d_h36m_gt.npz: contains 2D projections of those 3D poses into each camera view.\n",
    "\n",
    "These above files are actually created using code from VideoPose3D repository:https://github.com/facebookresearch/VideoPose3D/blob/1afb1ca0f1237776518469876342fc8669d3f6a9/data/prepare_data_h36m.py\n",
    "\n",
    "This script is a converter and preprocessor for the Human3.6M dataset, specifically designed for use with the VideoPose3D framework (by Facebook AI Research). It takes the raw Human3.6M data (in .h5, .mat, or .cdf formats) and generates two preprocessed .npz files.\n",
    "\n",
    "\n",
    "You must specify one source for the data (from archive, .mat, or .cdf).\n",
    "\n",
    "# üì¶ Human3.6M Dataset Conversion and Processing Pipeline from videopose3D\n",
    "\n",
    "This notebook explains how the Human3.6M dataset is converted to `.npz` files for 2D-to-3D pose estimation training, used in frameworks like **VideoPose3D**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üéõÔ∏è Argument Parsing\n",
    "\n",
    "The script takes in one of the following arguments:\n",
    "\n",
    "- `--from-archive`: Loads from Martinez et al.'s preprocessed `.h5` archive\n",
    "- `--from-source`: Uses `.mat` files converted from `.cdf` using MATLAB\n",
    "- `--from-source-cdf`: Directly loads raw `.cdf` files using Python's `cdflib`\n",
    "\n",
    "üí° You must provide **only one** of the arguments.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üìê Conversion to 3D Format (`data_3d_h36m.npz`)\n",
    "\n",
    "### üóÇÔ∏è Option A: `--from-archive`\n",
    "- Loads preprocessed `.h5` files containing 3D poses\n",
    "- Each action file has shape `[frames, 32, 3]`\n",
    "- Discards corrupted videos like `S11/Directions`\n",
    "- Saves everything in `data_3d_h36m.npz`\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Option B: `--from-source` (MATLAB `.mat` files)\n",
    "- Loads `.mat` files converted from original `.cdf`\n",
    "- Extracts the 3D pose data from the `\"data\"` field\n",
    "- Reshapes to `[frames, 32, 3]`\n",
    "- Converts from millimeters to meters\n",
    "- Saves as float32 in `data_3d_h36m.npz`\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Option C: `--from-source-cdf` (original `.cdf` files)\n",
    "- Loads the `\"Pose\"` variable using `cdflib`\n",
    "- Reshapes to `[frames, 32, 3]`\n",
    "- Converts values from millimeters to meters\n",
    "- Uses a consistent naming convention (e.g., `WalkingDog` ‚Üí `WalkDog`)\n",
    "- Structure of the final data:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'S1': {\n",
    "    'Walking': [frames, 32, 3],\n",
    "    'Sitting': ...\n",
    "  },\n",
    "  ...\n",
    "}\n",
    "```\n",
    "## 3. üéØ Conversion to 2D Format (`data_2d_h36m_gt.npz`)\n",
    "\n",
    "### üîÅ From 3D World Coordinates to 2D Pixel Coordinates\n",
    "\n",
    "For each 3D pose, the script projects it into each camera view using the following sequence:\n",
    "\n",
    "```python\n",
    "pos_3d = world_to_camera(...)\n",
    "pos_2d = project_to_2d(...)\n",
    "pos_2d_pixel = image_coordinates(...)\n",
    "```\n",
    "\n",
    "```python\n",
    "Format of 2D Data\n",
    "\n",
    "{\n",
    "  'S1': {\n",
    "    'Walking': [\n",
    "        [frames x 17 x 2]  # camera 1\n",
    "        [frames x 17 x 2]  # camera 2\n",
    "        [frames x 17 x 2]  # camera 3\n",
    "        [frames x 17 x 2]  # camera 4\n",
    "    ],\n",
    "    ...\n",
    "  },\n",
    "  ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23080b94-0eb2-4ea7-8117-3c4b771069a0",
   "metadata": {},
   "source": [
    "## Follow the steps here:\n",
    "\n",
    "I have took code from below repository:\n",
    "\n",
    "https://github.com/facebookresearch/VideoPose3D/blob/main/DATASETS.md\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df8344a-16d7-4c6f-992a-b4c55e940253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VideoPose3D/common/utils.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "def wrap(func, *args, unsqueeze=False):\n",
    "    \"\"\"\n",
    "    Wrap a torch function so it can be called with NumPy arrays.\n",
    "    Input and return types are seamlessly converted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert input types where applicable\n",
    "    args = list(args)\n",
    "    for i, arg in enumerate(args):\n",
    "        if type(arg) == np.ndarray:\n",
    "            args[i] = torch.from_numpy(arg)\n",
    "            if unsqueeze:\n",
    "                args[i] = args[i].unsqueeze(0)\n",
    "        \n",
    "    result = func(*args)\n",
    "    \n",
    "    # Convert output types where applicable\n",
    "    if isinstance(result, tuple):\n",
    "        result = list(result)\n",
    "        for i, res in enumerate(result):\n",
    "            if type(res) == torch.Tensor:\n",
    "                if unsqueeze:\n",
    "                    res = res.squeeze(0)\n",
    "                result[i] = res.numpy()\n",
    "        return tuple(result)\n",
    "    elif type(result) == torch.Tensor:\n",
    "        if unsqueeze:\n",
    "            result = result.squeeze(0)\n",
    "        return result.numpy()\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "def deterministic_random(min_value, max_value, data):\n",
    "    digest = hashlib.sha256(data.encode()).digest()\n",
    "    raw_value = int.from_bytes(digest[:4], byteorder='little', signed=False)\n",
    "    return int(raw_value / (2**32 - 1) * (max_value - min_value)) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428cd25b-c07c-40aa-809a-3269b3b78f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VideoPose3D/common/quaternion.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import torch\n",
    "\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "\n",
    "    qvec = q[..., 1:]\n",
    "    uv = torch.cross(qvec, v, dim=len(q.shape)-1)\n",
    "    uuv = torch.cross(qvec, uv, dim=len(q.shape)-1)\n",
    "    return (v + 2 * (q[..., :1] * uv + uuv))\n",
    "    \n",
    "    \n",
    "def qinverse(q, inplace=False):\n",
    "    # We assume the quaternion to be normalized\n",
    "    if inplace:\n",
    "        q[..., 1:] *= -1\n",
    "        return q\n",
    "    else:\n",
    "        w = q[..., :1]\n",
    "        xyz = q[..., 1:]\n",
    "        return torch.cat((w, -xyz), dim=len(q.shape)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09abbe59-29de-4a1e-aaf3-bd5cef4432a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VideoPose3D/common/camera.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from common.utils import wrap\n",
    "# from common.quaternion import qrot, qinverse\n",
    "\n",
    "def normalize_screen_coordinates(X, w, h): \n",
    "    assert X.shape[-1] == 2\n",
    "    \n",
    "    # Normalize so that [0, w] is mapped to [-1, 1], while preserving the aspect ratio\n",
    "    return X/w*2 - [1, h/w]\n",
    "\n",
    "    \n",
    "def image_coordinates(X, w, h):\n",
    "    assert X.shape[-1] == 2\n",
    "    \n",
    "    # Reverse camera frame normalization\n",
    "    return (X + [1, h/w])*w/2\n",
    "    \n",
    "\n",
    "def world_to_camera(X, R, t):\n",
    "    Rt = wrap(qinverse, R) # Invert rotation\n",
    "    return wrap(qrot, np.tile(Rt, (*X.shape[:-1], 1)), X - t) # Rotate and translate\n",
    "\n",
    "    \n",
    "def camera_to_world(X, R, t):\n",
    "    return wrap(qrot, np.tile(R, (*X.shape[:-1], 1)), X) + t\n",
    "\n",
    "    \n",
    "def project_to_2d(X, camera_params):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using the Human3.6M camera projection function.\n",
    "    This is a differentiable and batched reimplementation of the original MATLAB script.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 3D points in *camera space* to transform (N, *, 3)\n",
    "    camera_params -- intrinsic parameteres (N, 2+2+3+2=9)\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 3\n",
    "    assert len(camera_params.shape) == 2\n",
    "    assert camera_params.shape[-1] == 9\n",
    "    assert X.shape[0] == camera_params.shape[0]\n",
    "    \n",
    "    while len(camera_params.shape) < len(X.shape):\n",
    "        camera_params = camera_params.unsqueeze(1)\n",
    "        \n",
    "    f = camera_params[..., :2]\n",
    "    c = camera_params[..., 2:4]\n",
    "    k = camera_params[..., 4:7]\n",
    "    p = camera_params[..., 7:]\n",
    "    \n",
    "    XX = torch.clamp(X[..., :2] / X[..., 2:], min=-1, max=1)\n",
    "    r2 = torch.sum(XX[..., :2]**2, dim=len(XX.shape)-1, keepdim=True)\n",
    "\n",
    "    radial = 1 + torch.sum(k * torch.cat((r2, r2**2, r2**3), dim=len(r2.shape)-1), dim=len(r2.shape)-1, keepdim=True)\n",
    "    tan = torch.sum(p*XX, dim=len(XX.shape)-1, keepdim=True)\n",
    "\n",
    "    XXX = XX*(radial + tan) + p*r2\n",
    "    \n",
    "    return f*XXX + c\n",
    "\n",
    "def project_to_2d_linear(X, camera_params):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using only linear parameters (focal length and principal point).\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 3D points in *camera space* to transform (N, *, 3)\n",
    "    camera_params -- intrinsic parameteres (N, 2+2+3+2=9)\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 3\n",
    "    assert len(camera_params.shape) == 2\n",
    "    assert camera_params.shape[-1] == 9\n",
    "    assert X.shape[0] == camera_params.shape[0]\n",
    "    \n",
    "    while len(camera_params.shape) < len(X.shape):\n",
    "        camera_params = camera_params.unsqueeze(1)\n",
    "        \n",
    "    f = camera_params[..., :2]\n",
    "    c = camera_params[..., 2:4]\n",
    "    \n",
    "    XX = torch.clamp(X[..., :2] / X[..., 2:], min=-1, max=1)\n",
    "    \n",
    "    return f*XX + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d872e308-39c7-47ad-9f90-9551c382307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VideoPose3D/common/skeleton.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Skeleton:\n",
    "    def __init__(self, parents, joints_left, joints_right):\n",
    "        assert len(joints_left) == len(joints_right)\n",
    "        \n",
    "        self._parents = np.array(parents)\n",
    "        self._joints_left = joints_left\n",
    "        self._joints_right = joints_right\n",
    "        self._compute_metadata()\n",
    "    \n",
    "    def num_joints(self):\n",
    "        return len(self._parents)\n",
    "    \n",
    "    def parents(self):\n",
    "        return self._parents\n",
    "    \n",
    "    def has_children(self):\n",
    "        return self._has_children\n",
    "    \n",
    "    def children(self):\n",
    "        return self._children\n",
    "    \n",
    "    def remove_joints(self, joints_to_remove):\n",
    "        \"\"\"\n",
    "        Remove the joints specified in 'joints_to_remove'.\n",
    "        \"\"\"\n",
    "        valid_joints = []\n",
    "        for joint in range(len(self._parents)):\n",
    "            if joint not in joints_to_remove:\n",
    "                valid_joints.append(joint)\n",
    "\n",
    "        for i in range(len(self._parents)):\n",
    "            while self._parents[i] in joints_to_remove:\n",
    "                self._parents[i] = self._parents[self._parents[i]]\n",
    "                \n",
    "        index_offsets = np.zeros(len(self._parents), dtype=int)\n",
    "        new_parents = []\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if i not in joints_to_remove:\n",
    "                new_parents.append(parent - index_offsets[parent])\n",
    "            else:\n",
    "                index_offsets[i:] += 1\n",
    "        self._parents = np.array(new_parents)\n",
    "        \n",
    "        \n",
    "        if self._joints_left is not None:\n",
    "            new_joints_left = []\n",
    "            for joint in self._joints_left:\n",
    "                if joint in valid_joints:\n",
    "                    new_joints_left.append(joint - index_offsets[joint])\n",
    "            self._joints_left = new_joints_left\n",
    "        if self._joints_right is not None:\n",
    "            new_joints_right = []\n",
    "            for joint in self._joints_right:\n",
    "                if joint in valid_joints:\n",
    "                    new_joints_right.append(joint - index_offsets[joint])\n",
    "            self._joints_right = new_joints_right\n",
    "\n",
    "        self._compute_metadata()\n",
    "        \n",
    "        return valid_joints\n",
    "    \n",
    "    def joints_left(self):\n",
    "        return self._joints_left\n",
    "    \n",
    "    def joints_right(self):\n",
    "        return self._joints_right\n",
    "        \n",
    "    def _compute_metadata(self):\n",
    "        self._has_children = np.zeros(len(self._parents)).astype(bool)\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if parent != -1:\n",
    "                self._has_children[parent] = True\n",
    "\n",
    "        self._children = []\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            self._children.append([])\n",
    "        for i, parent in enumerate(self._parents):\n",
    "            if parent != -1:\n",
    "                self._children[parent].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc91e85-d5cf-4f20-8411-5b50655807e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VideoPose3D/common/mocap_dataset.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "# from common.skeleton import Skeleton\n",
    "\n",
    "class MocapDataset:\n",
    "    def __init__(self, fps, skeleton):\n",
    "        self._skeleton = skeleton\n",
    "        self._fps = fps\n",
    "        self._data = None # Must be filled by subclass\n",
    "        self._cameras = None # Must be filled by subclass\n",
    "    \n",
    "    def remove_joints(self, joints_to_remove):\n",
    "        kept_joints = self._skeleton.remove_joints(joints_to_remove)\n",
    "        for subject in self._data.keys():\n",
    "            for action in self._data[subject].keys():\n",
    "                s = self._data[subject][action]\n",
    "                if 'positions' in s:\n",
    "                    s['positions'] = s['positions'][:, kept_joints]\n",
    "                \n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "        \n",
    "    def subjects(self):\n",
    "        return self._data.keys()\n",
    "    \n",
    "    def fps(self):\n",
    "        return self._fps\n",
    "    \n",
    "    def skeleton(self):\n",
    "        return self._skeleton\n",
    "        \n",
    "    def cameras(self):\n",
    "        return self._cameras\n",
    "    \n",
    "    def supports_semi_supervised(self):\n",
    "        # This method can be overridden\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb080360-9b67-4b8a-ada1-c0e9695c6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VideoPose3D/common/h36m_dataset.py\n",
    "# Copyright (c) 2018-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "# from common.skeleton import Skeleton\n",
    "# from common.mocap_dataset import MocapDataset\n",
    "# from common.camera import normalize_screen_coordinates, image_coordinates\n",
    "       \n",
    "h36m_skeleton = Skeleton(parents=[-1,  0,  1,  2,  3,  4,  0,  6,  7,  8,  9,  0, 11, 12, 13, 14, 12,\n",
    "       16, 17, 18, 19, 20, 19, 22, 12, 24, 25, 26, 27, 28, 27, 30],\n",
    "       joints_left=[6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    "       joints_right=[1, 2, 3, 4, 5, 24, 25, 26, 27, 28, 29, 30, 31])\n",
    "\n",
    "h36m_cameras_intrinsic_params = [\n",
    "    {\n",
    "        'id': '54138969',\n",
    "        'center': [512.54150390625, 515.4514770507812],\n",
    "        'focal_length': [1145.0494384765625, 1143.7811279296875],\n",
    "        'radial_distortion': [-0.20709891617298126, 0.24777518212795258, -0.0030751503072679043],\n",
    "        'tangential_distortion': [-0.0009756988729350269, -0.00142447161488235],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1002,\n",
    "        'azimuth': 70, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '55011271',\n",
    "        'center': [508.8486328125, 508.0649108886719],\n",
    "        'focal_length': [1149.6756591796875, 1147.5916748046875],\n",
    "        'radial_distortion': [-0.1942136287689209, 0.2404085397720337, 0.006819975562393665],\n",
    "        'tangential_distortion': [-0.0016190266469493508, -0.0027408944442868233],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1000,\n",
    "        'azimuth': -70, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '58860488',\n",
    "        'center': [519.8158569335938, 501.40264892578125],\n",
    "        'focal_length': [1149.1407470703125, 1148.7989501953125],\n",
    "        'radial_distortion': [-0.2083381861448288, 0.25548800826072693, -0.0024604974314570427],\n",
    "        'tangential_distortion': [0.0014843869721516967, -0.0007599993259645998],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1000,\n",
    "        'azimuth': 110, # Only used for visualization\n",
    "    },\n",
    "    {\n",
    "        'id': '60457274',\n",
    "        'center': [514.9682006835938, 501.88201904296875],\n",
    "        'focal_length': [1145.5113525390625, 1144.77392578125],\n",
    "        'radial_distortion': [-0.198384091258049, 0.21832367777824402, -0.008947807364165783],\n",
    "        'tangential_distortion': [-0.0005872055771760643, -0.0018133620033040643],\n",
    "        'res_w': 1000,\n",
    "        'res_h': 1002,\n",
    "        'azimuth': -110, # Only used for visualization\n",
    "    },\n",
    "]\n",
    "\n",
    "h36m_cameras_extrinsic_params = {\n",
    "    'S1': [\n",
    "        {\n",
    "            'orientation': [0.1407056450843811, -0.1500701755285263, -0.755240797996521, 0.6223280429840088],\n",
    "            'translation': [1841.1070556640625, 4955.28466796875, 1563.4454345703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6157187819480896, -0.764836311340332, -0.14833825826644897, 0.11794740706682205],\n",
    "            'translation': [1761.278564453125, -5078.0068359375, 1606.2650146484375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14651472866535187, -0.14647851884365082, 0.7653023600578308, -0.6094175577163696],\n",
    "            'translation': [-1846.7777099609375, 5215.04638671875, 1491.972412109375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5834008455276489, -0.7853162288665771, 0.14548823237419128, -0.14749594032764435],\n",
    "            'translation': [-1794.7896728515625, -3722.698974609375, 1574.8927001953125],\n",
    "        },\n",
    "    ],\n",
    "    'S2': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S3': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S4': [\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    ],\n",
    "    'S5': [\n",
    "        {\n",
    "            'orientation': [0.1467377245426178, -0.162370964884758, -0.7551892995834351, 0.6178938746452332],\n",
    "            'translation': [2097.3916015625, 4880.94482421875, 1605.732421875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6159758567810059, -0.7626792192459106, -0.15728192031383514, 0.1189815029501915],\n",
    "            'translation': [2031.7008056640625, -5167.93310546875, 1612.923095703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14291371405124664, -0.12907841801643372, 0.7678384780883789, -0.6110143065452576],\n",
    "            'translation': [-1620.5948486328125, 5171.65869140625, 1496.43701171875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5920479893684387, -0.7814217805862427, 0.1274748593568802, -0.15036417543888092],\n",
    "            'translation': [-1637.1737060546875, -3867.3173828125, 1547.033203125],\n",
    "        },\n",
    "    ],\n",
    "    'S6': [\n",
    "        {\n",
    "            'orientation': [0.1337897777557373, -0.15692396461963654, -0.7571090459823608, 0.6198879480361938],\n",
    "            'translation': [1935.4517822265625, 4950.24560546875, 1618.0838623046875],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6147197484970093, -0.7628812789916992, -0.16174767911434174, 0.11819244921207428],\n",
    "            'translation': [1969.803955078125, -5128.73876953125, 1632.77880859375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.1529948115348816, -0.13529130816459656, 0.7646096348762512, -0.6112781167030334],\n",
    "            'translation': [-1769.596435546875, 5185.361328125, 1476.993408203125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5916101336479187, -0.7804774045944214, 0.12832270562648773, -0.1561593860387802],\n",
    "            'translation': [-1721.668701171875, -3884.13134765625, 1540.4879150390625],\n",
    "        },\n",
    "    ],\n",
    "    'S7': [\n",
    "        {\n",
    "            'orientation': [0.1435241848230362, -0.1631336808204651, -0.7548328638076782, 0.6188824772834778],\n",
    "            'translation': [1974.512939453125, 4926.3544921875, 1597.8326416015625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6141672730445862, -0.7638262510299683, -0.1596645563840866, 0.1177929937839508],\n",
    "            'translation': [1937.0584716796875, -5119.7900390625, 1631.5665283203125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14550060033798218, -0.12874816358089447, 0.7660516500473022, -0.6127139329910278],\n",
    "            'translation': [-1741.8111572265625, 5208.24951171875, 1464.8245849609375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5912848114967346, -0.7821764349937439, 0.12445473670959473, -0.15196487307548523],\n",
    "            'translation': [-1734.7105712890625, -3832.42138671875, 1548.5830078125],\n",
    "        },\n",
    "    ],\n",
    "    'S8': [\n",
    "        {\n",
    "            'orientation': [0.14110587537288666, -0.15589867532253265, -0.7561917304992676, 0.619644045829773],\n",
    "            'translation': [2150.65185546875, 4896.1611328125, 1611.9046630859375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6169601678848267, -0.7647668123245239, -0.14846350252628326, 0.11158157885074615],\n",
    "            'translation': [2219.965576171875, -5148.453125, 1613.0440673828125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.1471444070339203, -0.13377119600772858, 0.7670128345489502, -0.6100369691848755],\n",
    "            'translation': [-1571.2215576171875, 5137.0185546875, 1498.1761474609375],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5927824378013611, -0.7825870513916016, 0.12147816270589828, -0.14631995558738708],\n",
    "            'translation': [-1476.913330078125, -3896.7412109375, 1547.97216796875],\n",
    "        },\n",
    "    ],\n",
    "    'S9': [\n",
    "        {\n",
    "            'orientation': [0.15540587902069092, -0.15548215806484222, -0.7532095313072205, 0.6199594736099243],\n",
    "            'translation': [2044.45849609375, 4935.1171875, 1481.2275390625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.618784487247467, -0.7634735107421875, -0.14132238924503326, 0.11933968216180801],\n",
    "            'translation': [1990.959716796875, -5123.810546875, 1568.8048095703125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.13357827067375183, -0.1367100477218628, 0.7689454555511475, -0.6100738644599915],\n",
    "            'translation': [-1670.9921875, 5211.98583984375, 1528.387939453125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5879399180412292, -0.7823407053947449, 0.1427614390850067, -0.14794869720935822],\n",
    "            'translation': [-1696.04345703125, -3827.099853515625, 1591.4127197265625],\n",
    "        },\n",
    "    ],\n",
    "    'S11': [\n",
    "        {\n",
    "            'orientation': [0.15232472121715546, -0.15442320704460144, -0.7547563314437866, 0.6191070079803467],\n",
    "            'translation': [2098.440185546875, 4926.5546875, 1500.278564453125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.6189449429512024, -0.7600917220115662, -0.15300633013248444, 0.1255258321762085],\n",
    "            'translation': [2083.182373046875, -4912.1728515625, 1561.07861328125],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.14943228662014008, -0.15650227665901184, 0.7681233882904053, -0.6026304364204407],\n",
    "            'translation': [-1609.8153076171875, 5177.3359375, 1537.896728515625],\n",
    "        },\n",
    "        {\n",
    "            'orientation': [0.5894251465797424, -0.7818877100944519, 0.13991211354732513, -0.14715361595153809],\n",
    "            'translation': [-1590.738037109375, -3854.1689453125, 1578.017578125],\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "class Human36mDataset(MocapDataset):\n",
    "    def __init__(self, path, remove_static_joints=True):\n",
    "        super().__init__(fps=50, skeleton=h36m_skeleton)\n",
    "        \n",
    "        self._cameras = copy.deepcopy(h36m_cameras_extrinsic_params)\n",
    "        for cameras in self._cameras.values():\n",
    "            for i, cam in enumerate(cameras):\n",
    "                cam.update(h36m_cameras_intrinsic_params[i])\n",
    "                for k, v in cam.items():\n",
    "                    if k not in ['id', 'res_w', 'res_h']:\n",
    "                        cam[k] = np.array(v, dtype='float32')\n",
    "                \n",
    "                # Normalize camera frame\n",
    "                cam['center'] = normalize_screen_coordinates(cam['center'], w=cam['res_w'], h=cam['res_h']).astype('float32')\n",
    "                cam['focal_length'] = cam['focal_length']/cam['res_w']*2\n",
    "                if 'translation' in cam:\n",
    "                    cam['translation'] = cam['translation']/1000 # mm to meters\n",
    "                \n",
    "                # Add intrinsic parameters vector\n",
    "                cam['intrinsic'] = np.concatenate((cam['focal_length'],\n",
    "                                                   cam['center'],\n",
    "                                                   cam['radial_distortion'],\n",
    "                                                   cam['tangential_distortion']))\n",
    "        \n",
    "        # Load serialized dataset\n",
    "        data = np.load(path, allow_pickle=True)['positions_3d'].item()\n",
    "        \n",
    "        self._data = {}\n",
    "        for subject, actions in data.items():\n",
    "            self._data[subject] = {}\n",
    "            for action_name, positions in actions.items():\n",
    "                self._data[subject][action_name] = {\n",
    "                    'positions': positions,\n",
    "                    'cameras': self._cameras[subject],\n",
    "                }\n",
    "                \n",
    "        if remove_static_joints:\n",
    "            # Bring the skeleton to 17 joints instead of the original 32\n",
    "            self.remove_joints([4, 5, 9, 10, 11, 16, 20, 21, 22, 23, 24, 28, 29, 30, 31])\n",
    "            \n",
    "            # Rewire shoulders to the correct parents\n",
    "            self._skeleton._parents[11] = 8\n",
    "            self._skeleton._parents[14] = 8\n",
    "            \n",
    "    def supports_semi_supervised(self):\n",
    "        return True\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf25c1-e854-480c-9ea8-d772e63813d9",
   "metadata": {},
   "source": [
    "## Generate 3d file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb50889-f976-435c-99eb-2afe5a6aa6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_3d_h36m.npz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate 3d numpy file.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cdflib\n",
    "\n",
    "# === SETTINGS ===\n",
    "subjects = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11']\n",
    "dataset_root = \"C:/Users/dataset/human36\"\n",
    "output_filename_3d = \"data_3d_h36m.npz\"\n",
    "\n",
    "# === 1. Convert CDF to 3D pose dictionary ===\n",
    "output_3d = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_path = os.path.join(dataset_root, subject, 'MyPoseFeatures', 'D3_Positions')\n",
    "    cdf_files = sorted(glob(os.path.join(subject_path, '*.cdf')))\n",
    "    \n",
    "    output_3d[subject] = {}\n",
    "    \n",
    "    for file_path in cdf_files:\n",
    "        action = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        if subject == 'S11' and action == 'Directions':\n",
    "            continue\n",
    "        \n",
    "        cdf = cdflib.CDF(file_path)\n",
    "        pose = cdf['Pose']  # shape (1, N, 96)\n",
    "        pose = pose.squeeze().reshape(-1, 32, 3) / 1000.0  # meters\n",
    "        canonical_name = action.replace('TakingPhoto', 'Photo').replace('WalkingDog', 'WalkDog')\n",
    "        output_3d[subject][canonical_name] = pose.astype(np.float32)\n",
    "\n",
    "# Save as compressed .npz\n",
    "np.savez_compressed(output_filename_3d, positions_3d=output_3d)\n",
    "# print(f\"Saved: {output_filename_3d}\")\n",
    "output_filename_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88efcf-de3e-4b88-932f-a8ccac9b983d",
   "metadata": {},
   "source": [
    "## Generate 2d file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feefeca3-2b1f-4b84-8fec-bcd5bf5a6179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_2d_h36m_gt.npz\n"
     ]
    }
   ],
   "source": [
    "#To generate 2d file from 3d\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# from common.h36m_dataset import Human36mDataset\n",
    "# from common.camera import world_to_camera, project_to_2d, image_coordinates\n",
    "# from common.utils import wrap\n",
    "\n",
    "# === Configuration ===\n",
    "input_3d_file = 'data_3d_h36m.npz'         # already generated by you\n",
    "output_2d_file = 'data_2d_h36m_gt.npz'     # this will be created\n",
    "\n",
    "# === Load 3D data ===\n",
    "dataset = Human36mDataset(input_3d_file)\n",
    "output_2d_poses = {}\n",
    "\n",
    "# === Project each 3D pose into each camera view ===\n",
    "for subject in dataset.subjects():\n",
    "    output_2d_poses[subject] = {}\n",
    "    \n",
    "    for action in dataset[subject].keys():\n",
    "        anim = dataset[subject][action]\n",
    "        \n",
    "        positions_2d = []\n",
    "        for cam in anim['cameras']:\n",
    "            # Step 1: convert world 3D ‚Üí camera 3D\n",
    "            pos_3d = world_to_camera(anim['positions'], R=cam['orientation'], t=cam['translation'])\n",
    "\n",
    "            # Step 2: project to 2D using intrinsic parameters\n",
    "            pos_2d = wrap(project_to_2d, pos_3d, cam['intrinsic'], unsqueeze=True)\n",
    "\n",
    "            # Step 3: convert to image coordinates (pixel space)\n",
    "            pos_2d_pixel_space = image_coordinates(pos_2d, w=cam['res_w'], h=cam['res_h'])\n",
    "\n",
    "            positions_2d.append(pos_2d_pixel_space.astype('float32'))\n",
    "\n",
    "        output_2d_poses[subject][action] = positions_2d\n",
    "\n",
    "# === Save output file ===\n",
    "metadata = {\n",
    "    'num_joints': dataset.skeleton().num_joints(),\n",
    "    'keypoints_symmetry': [dataset.skeleton().joints_left(), dataset.skeleton().joints_right()]\n",
    "}\n",
    "\n",
    "np.savez_compressed(output_2d_file, positions_2d=output_2d_poses, metadata=metadata)\n",
    "print(f\"Saved: {output_2d_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f3c49-91e8-48dc-93ca-c63dd6c8d990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
